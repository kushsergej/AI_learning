{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df29563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interact with LLM via API authn\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "try:\n",
    "    # This will tell you if your API key is valid and show account info\n",
    "    models = client.models.list()\n",
    "    print(f\"Available models: {len(models.data)}\")\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful DevOps assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Explain how CI/CD works in Azure DevOps.\"}\n",
    "        ]\n",
    "    )\n",
    "    print(response.choices[0].message.content)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3046c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sigmoid function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the function\n",
    "x = np.linspace(-10, 10, 400)\n",
    "y = 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x, y, linewidth=2)\n",
    "plt.title(r\"Plot of $f(x) = \\frac{1}{1 + e^{-x}}$ (Sigmoid Function)\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fde1e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Load LLM\n",
      ">>> Tokenize loaded datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 26732/26732 [00:14<00:00, 1821.76 examples/s]\n",
      "Map: 100%|██████████| 3432/3432 [00:01<00:00, 1977.08 examples/s]\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Training\n",
      "Expected input batch_size (4096) to match target batch_size (4).\n"
     ]
    }
   ],
   "source": [
    "# LoRA fine-tuning\n",
    "\n",
    "import hf_xet\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from datasets import load_dataset\n",
    "\n",
    "try:\n",
    "    print('>>> Load LLM')\n",
    "    model_name = 'gpt2'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto')\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        fan_in_fan_out=True,\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "    def tokenize_fn(examples):\n",
    "        return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "    print('>>> Tokenize loaded datasets')\n",
    "    dataset = load_dataset('mteb/tweet_sentiment_extraction')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenized_datasets = dataset.map(tokenize_fn, batched=True)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./LoRA_output\",\n",
    "        per_device_train_batch_size=4,\n",
    "        num_train_epochs=3,\n",
    "        save_steps=100,\n",
    "        seed=42,\n",
    "        fp16=True,\n",
    "        dataloader_pin_memory=False,\n",
    "        dataloader_num_workers=0\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        processing_class=tokenizer,\n",
    "        train_dataset=tokenized_datasets['train']\n",
    "    )\n",
    "\n",
    "    print('>>> Training')\n",
    "    trainer.train()\n",
    "\n",
    "    print('>>> Validating')\n",
    "    trainer.evaluate()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
