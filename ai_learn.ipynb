{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d16d64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugginface playground\n",
    "\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from adapters import AutoAdapterModel, list_adapters\n",
    "\n",
    "login(token=os.getenv('HF_READ_TOKEN'))\n",
    "model_id = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'\n",
    "\n",
    "# ls -la /c/Users/Siarhei_Kushniaruk/.cache/huggingface\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map='auto')\n",
    "model.eval()\n",
    "\n",
    "adapter_infos = list_adapters(model_name=model_id)\n",
    "for adapter_info in adapter_infos:\n",
    "    print('Id:', adapter_info.adapter_id)\n",
    "    print('Model name:', adapter_info.model_name)\n",
    "    print('Uploaded by:', adapter_info.username)\n",
    "# model = AutoAdapterModel.from_pretrained(model_id)\n",
    "# adapter_name = model.load_adapter('AdapterHub/roberta-base-pf-imdb', source='hf')\n",
    "# model.active_adapters = adapter_name\n",
    "\n",
    "prompt = '''\n",
    "<|system|> You are a experienced DevOps Engineer with robust skills in Azure and Python </s>\\\n",
    "<|user|> How to write a unit tests on Python project? </s>\\\n",
    "<|assistant|>\n",
    "'''\n",
    "\n",
    "print(f'tokens: {tokenizer.tokenize(prompt)}')\n",
    "print(f'ids: {tokenizer.convert_tokens_to_ids(tokenizer.tokenize(prompt))}')\n",
    "print(f'summary: {tokenizer(prompt).to('cpu')}')\n",
    "print(f'decoded: {tokenizer.decode(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(prompt)))}')\n",
    "\n",
    "pipe = pipeline(task='text-generation', model=model, tokenizer=tokenizer)\n",
    "result = pipe(\n",
    "    prompt,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    return_full_text=False\n",
    ")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6454e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bitsandbytes quantization\n",
    "\n",
    "import os\n",
    "import bitsandbytes\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "\n",
    "login(token=os.getenv('HF_READ_TOKEN'))\n",
    "model_id = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "# ls -la /c/Users/Siarhei_Kushniaruk/.cache/huggingface\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "model_4bit = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config, device_map='auto')\n",
    "\n",
    "prompt = '''\n",
    "<|system|> You are a experienced DevOps Engineer with robust skills in Azure and Python </s>\\\n",
    "<|user|> How to write a unit tests on Python project? </s>\\\n",
    "<|assistant|>\n",
    "'''\n",
    "\n",
    "pipe = pipeline(task='text-generation', model=model_4bit, tokenizer=tokenizer)\n",
    "result = pipe(\n",
    "    prompt,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    return_full_text=False\n",
    ")\n",
    "print(f'{[model_4bit.get_memory_footprint()]} >>> {result[0]['generated_text']}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3046c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sigmoid function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the function\n",
    "x = np.linspace(-10, 10, 400)\n",
    "y = 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x, y, linewidth=2)\n",
    "plt.title(r'Plot of $f(x) = \\frac{1}{1 + e^{-x}}$ (Sigmoid Function)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fde1e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA fine-tuning\n",
    "\n",
    "import hf_xet\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from datasets import load_dataset\n",
    "\n",
    "try:\n",
    "    print('>>> Load LLM')\n",
    "    model_name = 'gpt2'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto')\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        fan_in_fan_out=True,\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "    def tokenize_fn(examples):\n",
    "        return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "    print('>>> Tokenize loaded datasets')\n",
    "    dataset = load_dataset('mteb/tweet_sentiment_extraction')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenized_datasets = dataset.map(tokenize_fn, batched=True)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./LoRA_output',\n",
    "        per_device_train_batch_size=4,\n",
    "        num_train_epochs=3,\n",
    "        save_steps=100,\n",
    "        seed=42,\n",
    "        fp16=True,\n",
    "        dataloader_pin_memory=False,\n",
    "        dataloader_num_workers=0\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        processing_class=tokenizer,\n",
    "        train_dataset=tokenized_datasets['train']\n",
    "    )\n",
    "\n",
    "    print('>>> Training')\n",
    "    trainer.train()\n",
    "\n",
    "    print('>>> Validating')\n",
    "    trainer.evaluate()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
