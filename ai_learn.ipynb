{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d16d64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugginface playground\n",
    "\n",
    "from huggingface_hub import login\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "login(token=os.getenv('HF_READ_TOKEN'))\n",
    "model_id = 'ibm-granite/granite-4.0-h-350m'\n",
    "\n",
    "# ls -la /c/Users/Siarhei_Kushniaruk/.cache/huggingface\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "model.eval()\n",
    "\n",
    "message = 'Moh is obviously my lovely cat'\n",
    "print('tokens: ', tokenizer.tokenize(message))\n",
    "print('ids: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(message)))\n",
    "print('decoded: ', tokenizer.decode(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(message))))\n",
    "\n",
    "chat = [\n",
    "    { 'role': 'user', 'content': 'I have to change the project' },\n",
    "    { 'role': 'user', 'content': 'More focused on AI' }\n",
    "]\n",
    "pipe = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "res = pipe(chat, temperature=0.7)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3046c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sigmoid function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the function\n",
    "x = np.linspace(-10, 10, 400)\n",
    "y = 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x, y, linewidth=2)\n",
    "plt.title(r'Plot of $f(x) = \\frac{1}{1 + e^{-x}}$ (Sigmoid Function)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fde1e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA fine-tuning\n",
    "\n",
    "import hf_xet\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from datasets import load_dataset\n",
    "\n",
    "try:\n",
    "    print('>>> Load LLM')\n",
    "    model_name = 'gpt2'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto')\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        fan_in_fan_out=True,\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "    def tokenize_fn(examples):\n",
    "        return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "    print('>>> Tokenize loaded datasets')\n",
    "    dataset = load_dataset('mteb/tweet_sentiment_extraction')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenized_datasets = dataset.map(tokenize_fn, batched=True)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./LoRA_output',\n",
    "        per_device_train_batch_size=4,\n",
    "        num_train_epochs=3,\n",
    "        save_steps=100,\n",
    "        seed=42,\n",
    "        fp16=True,\n",
    "        dataloader_pin_memory=False,\n",
    "        dataloader_num_workers=0\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        processing_class=tokenizer,\n",
    "        train_dataset=tokenized_datasets['train']\n",
    "    )\n",
    "\n",
    "    print('>>> Training')\n",
    "    trainer.train()\n",
    "\n",
    "    print('>>> Validating')\n",
    "    trainer.evaluate()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007ce363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GGUF quantization\n",
    "\n",
    "\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.quantization as quant\n",
    "\n",
    "# Step 1: Define a simple neural network model in PyTorch\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 50)  # First fully connected layer\n",
    "        self.fc2 = nn.Linear(50, 20)  # Second fully connected layer\n",
    "        self.fc3 = nn.Linear(20, 5)   # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))   # ReLU activation after first layer\n",
    "        x = torch.relu(self.fc2(x))   # ReLU activation after second layer\n",
    "        x = self.fc3(x)               # Output layer\n",
    "        return x\n",
    "\n",
    "# Step 2: Initialize the model and switch to evaluation mode\n",
    "model = SimpleModel()\n",
    "model.eval()\n",
    "\n",
    "\n",
    "torch.save(model, './LLMs/simple_model.pth')\n",
    "\n",
    "quantized_model = torch.ao.quantization.quantize_dynamic(\n",
    "    model, {nn.Linear},\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "torch.save(quantized_model, './LLMs/quantized_simple_model.pth')\n",
    "\n",
    "\n",
    "dummy_input = torch.randn(1, 10)\n",
    "print('Dummy input', dummy_input)\n",
    "print('Model output:', model(dummy_input))\n",
    "print('Quantized model output:', quantized_model(dummy_input))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
