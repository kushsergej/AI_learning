{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d16d64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: ['Ċ', '<', '|', 'system', '|', '>', 'ĠYou', 'Ġare', 'Ġa', 'Ġexperienced', 'ĠDev', 'Ops', 'ĠEngineer', 'Ġwith', 'Ġrobust', 'Ġskills', 'Ġin', 'ĠAzure', 'Ġand', 'ĠPython', 'Ġ</', 's', '><', '|', 'user', '|', '>', 'ĠHow', 'Ġto', 'Ġwrite', 'Ġa', 'Ġunit', 'Ġtests', 'Ġon', 'ĠPython', 'Ġproject', '?', 'Ġ</', 's', '><', '|', 'assistant', '|', '>Ċ', 'Ċ', '<', '|', 'system', '|', '>', 'ĠYou', 'Ġare', 'Ġa', 'Ġexperienced', 'ĠDev', 'Ops', 'ĠEngineer', 'Ġwith', 'Ġrobust', 'Ġskills', 'Ġin', 'ĠAzure', 'Ġand', 'ĠPython', 'Ġ</', 's', '><', '|', 'user', '|', '>', 'ĠOnce', 'Ġupon', 'Ġa', 'Ġtime', '...', 'Ġ</', 's', '><', '|', 'assistant', '|', '>Ċ']\n",
      "ids: [198, 27, 91, 8948, 91, 29, 1446, 525, 264, 10321, 6040, 38904, 28383, 448, 21765, 7361, 304, 34119, 323, 13027, 690, 82, 1784, 91, 872, 91, 29, 2585, 311, 3270, 264, 4982, 7032, 389, 13027, 2390, 30, 690, 82, 1784, 91, 77091, 91, 397, 198, 27, 91, 8948, 91, 29, 1446, 525, 264, 10321, 6040, 38904, 28383, 448, 21765, 7361, 304, 34119, 323, 13027, 690, 82, 1784, 91, 872, 91, 29, 9646, 5193, 264, 882, 1112, 690, 82, 1784, 91, 77091, 91, 397]\n",
      "summary: {'input_ids': [[151646, 198, 27, 91, 8948, 91, 29, 1446, 525, 264, 10321, 6040, 38904, 28383, 448, 21765, 7361, 304, 34119, 323, 13027, 690, 82, 1784, 91, 872, 91, 29, 2585, 311, 3270, 264, 4982, 7032, 389, 13027, 2390, 30, 690, 82, 1784, 91, 77091, 91, 397], [151646, 198, 27, 91, 8948, 91, 29, 1446, 525, 264, 10321, 6040, 38904, 28383, 448, 21765, 7361, 304, 34119, 323, 13027, 690, 82, 1784, 91, 872, 91, 29, 9646, 5193, 264, 882, 1112, 690, 82, 1784, 91, 77091, 91, 397]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "decoded: \n",
      "<|system|> You are a experienced DevOps Engineer with robust skills in Azure and Python </s><|user|> How to write a unit tests on Python project? </s><|assistant|>\n",
      "\n",
      "<|system|> You are a experienced DevOps Engineer with robust skills in Azure and Python </s><|user|> Once upon a time... </s><|assistant|>\n",
      "\n",
      "</think>\n",
      "\n",
      "To write unit tests for a Python project, follow these steps:\n",
      "\n",
      "1. **Define Test Cases**: Identify the functionalities you want to test and create test cases that cover all possible scenarios.\n",
      "\n",
      "2. **Use a Testing Framework**: Utilize Python's built-in testing framework, such as `unittest` or `pytest`, which provide structured testing capabilities.\n",
      "\n",
      "3. **Write Test Classes**: Create test classes for each module or function you want to test. These classes will contain test methods.\n",
      "\n",
      "4. **Implement Test Methods**: For each test method, write code that asserts the expected behavior of the function. Use assertions like `self.assertEqual`, `self assert`, etc.\n",
      "\n",
      "5. **Test Data**: Provide test data for your functions. This data can be passed into the function to test its behavior.\n",
      "\n",
      "6. **Run Tests**: Use tools like `pytest` or `unittest` to run your tests and collect results.\n",
      "\n",
      "7. **Handle Exceptions**: Test your functions to ensure they handle exceptions gracefully.\n",
      "\n",
      "8. **Automate Tests**: Use tools like `pytest` or `unittest` to run your tests automatically.\n",
      "\n",
      "9. **Document Tests**: Keep your tests organized and well-documented so that anyone reading them knows what each test does.\n",
      "\n",
      "10. **Test Output**: Review\n",
      "<|system|> You are a experienced DevOps Engineer with robust skills in Azure and Python </s><|user|> Once upon a time... </s><|assistant|>\n",
      "<|system|> You are a experienced DevOps Engineer with robust skills in Azure and Python </s><|user|> Once upon a time... </s><|assistant|>\n",
      "<|system|> You are a experienced DevOps Engineer with robust skills in Azure and Python </s><|user|> Once upon a time... </s><|assistant|>\n",
      "<|system|> You are a experienced DevOps Engineer with robust skills in Azure and Python </s><|user|> Once upon a time... </s><|assistant|>\n",
      "<|system|> You are a experienced DevOps Engineer with robust skills in Azure and Python </s><|user|> Once upon a time... </s><|assistant|>\n",
      "<|system|> You are a experienced DevOps Engineer with robust skills in Azure and Python </s><|user|> Once upon a time... </s><|assistant|>\n",
      "<|system|> You are a experienced DevOps Engineer with robust skills in Azure and Python </s><|user|> Once upon\n"
     ]
    }
   ],
   "source": [
    "# Hugginface playground\n",
    "\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from adapters import AutoAdapterModel, list_adapters\n",
    "\n",
    "login(token=os.getenv('HF_READ_TOKEN'))\n",
    "model_id = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'\n",
    "\n",
    "# ls -la /c/Users/Siarhei_Kushniaruk/.cache/huggingface\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map='auto')\n",
    "model.eval()\n",
    "\n",
    "adapter_infos = list_adapters(model_name=model_id)\n",
    "for adapter_info in adapter_infos:\n",
    "    print('Id:', adapter_info.adapter_id)\n",
    "    print('Model name:', adapter_info.model_name)\n",
    "    print('Uploaded by:', adapter_info.username)\n",
    "# model = AutoAdapterModel.from_pretrained(model_id)\n",
    "# adapter_name = model.load_adapter('AdapterHub/roberta-base-pf-imdb', source='hf')\n",
    "# model.active_adapters = adapter_name\n",
    "\n",
    "prompt = [\n",
    "'''\n",
    "<|system|> You are a experienced DevOps Engineer with robust skills in Azure and Python </s>\\\n",
    "<|user|> How to write a unit tests on Python project? </s>\\\n",
    "<|assistant|>\n",
    "''',\n",
    "'''\n",
    "<|system|> You are a experienced DevOps Engineer with robust skills in Azure and Python </s>\\\n",
    "<|user|> Once upon a time... </s>\\\n",
    "<|assistant|>\n",
    "'''\n",
    "]\n",
    "\n",
    "print(f'tokens: {tokenizer.tokenize(prompt)}')\n",
    "print(f'ids: {tokenizer.convert_tokens_to_ids(tokenizer.tokenize(prompt))}')\n",
    "print(f'summary: {tokenizer(prompt).to('cpu')}')\n",
    "print(f'decoded: {tokenizer.decode(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(prompt)))}')\n",
    "\n",
    "pipe = pipeline(task='text-generation', model=model, tokenizer=tokenizer)\n",
    "result = pipe(\n",
    "    prompt,\n",
    "    temperature=0.5,\n",
    "    top_p=0.85,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    return_full_text=False\n",
    ")\n",
    "\n",
    "for reply in result:\n",
    "    print(f'>>> {reply[0]['generated_text']}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6454e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bitsandbytes quantization\n",
    "\n",
    "import os\n",
    "import bitsandbytes\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "\n",
    "login(token=os.getenv('HF_READ_TOKEN'))\n",
    "model_id = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "# ls -la /c/Users/Siarhei_Kushniaruk/.cache/huggingface\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "model_4bit = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config, device_map='auto')\n",
    "\n",
    "prompt = '''\n",
    "<|system|> You are a experienced DevOps Engineer with robust skills in Azure and Python </s>\\\n",
    "<|user|> How to write a unit tests on Python project? </s>\\\n",
    "<|assistant|>\n",
    "'''\n",
    "\n",
    "pipe = pipeline(task='text-generation', model=model_4bit, tokenizer=tokenizer)\n",
    "result = pipe(\n",
    "    prompt,\n",
    "    temperature=0.5,\n",
    "    top_p=0.85,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    return_full_text=False\n",
    ")\n",
    "print(f'{[model_4bit.get_memory_footprint()]} >>> {result[0]['generated_text']}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3046c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sigmoid function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the function\n",
    "x = np.linspace(-10, 10, 400)\n",
    "y = 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x, y, linewidth=2)\n",
    "plt.title(r'Plot of $f(x) = \\frac{1}{1 + e^{-x}}$ (Sigmoid Function)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1057532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vLLM usage when using quantized model\n",
    "\n",
    "from vllm import LLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "login(token=os.getenv('HF_READ_TOKEN'))\n",
    "model_id = 'ibm-granite/granite-4.0-1b'\n",
    "\n",
    "# ls -la /c/Users/Siarhei_Kushniaruk/.cache/huggingface\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "\n",
    "engine = LLM(\n",
    "    model=model_id,\n",
    "    max_num_seqs=8,\n",
    "    max_num_batched_tokens=512,\n",
    "    dtype=\"float16\",\n",
    "    gpu_memory_utilization=0.9,\n",
    "    enforce_eager=False,\n",
    "    enable_chunked_prefill=True\n",
    ")\n",
    "\n",
    "prompt = 'Once upon a time'\n",
    "\n",
    "result = engine.generate(prompt, max_tokens=256)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fde1e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLoRA fine-tuning\n",
    "\n",
    "import hf_xet\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from datasets import load_dataset\n",
    "\n",
    "try:\n",
    "    print('>>> Load LLM')\n",
    "    model_name = 'gpt2'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto')\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        fan_in_fan_out=True,\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "    def tokenize_fn(examples):\n",
    "        return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "    print('>>> Tokenize loaded datasets')\n",
    "    dataset = load_dataset('mteb/tweet_sentiment_extraction')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenized_datasets = dataset.map(tokenize_fn, batched=True)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./LoRA_output',\n",
    "        per_device_train_batch_size=4,\n",
    "        num_train_epochs=3,\n",
    "        save_steps=100,\n",
    "        seed=42,\n",
    "        fp16=True,\n",
    "        dataloader_pin_memory=False,\n",
    "        dataloader_num_workers=0\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        processing_class=tokenizer,\n",
    "        train_dataset=tokenized_datasets['train']\n",
    "    )\n",
    "\n",
    "    print('>>> Training')\n",
    "    trainer.train()\n",
    "\n",
    "    print('>>> Validating')\n",
    "    trainer.evaluate()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
